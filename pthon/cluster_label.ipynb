{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d572a-a3ca-4235-b985-3bdd262f9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /opt/homebrew/opt/ollama/bin/ollama serve\n",
    "%pip install -q --isolated openreview-py PyPDF2 chromadb\n",
    "import ollama\n",
    "import openreview\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import PyPDF2\n",
    "import requests\n",
    "\n",
    "# API V2\n",
    "email = \"\"\n",
    "client = openreview.api.OpenReviewClient(  # type: ignore\n",
    "    baseurl=\"https://api2.openreview.net\", username=email, password=\"\"\n",
    ")\n",
    "venue_id = \"ICML.cc/2024/Conference\"\n",
    "venue_group = client.get_group(venue_id)\n",
    "review_name = venue_group.content[\"review_name\"][\"value\"]\n",
    "submission_name = venue_group.content[\"submission_name\"][\"value\"]\n",
    "\n",
    "\n",
    "def get_submissions():\n",
    "    submissions = client.get_all_notes(content={\"venueid\": venue_id}, details=\"replies\")\n",
    "    return submissions\n",
    "\n",
    "\n",
    "submissions = get_submissions()\n",
    "\n",
    "\n",
    "def remove_surrogates(text):\n",
    "    return re.sub(r\"[\\ud800-\\udfff]\", \"\", text)\n",
    "\n",
    "\n",
    "def download_pdf(pdf_link):\n",
    "    response = requests.get(pdf_link)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download the PDF. Status code: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_pdf_text(pdf_file):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    text = []\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        page_obj = pdf_reader.pages[page]\n",
    "        text.append(page_obj.extract_text())\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def find_references_start(parsed_text):\n",
    "    patterns = [\n",
    "        r\"(?i)(\\n|\\r\\n|\\r|\\.\\s|-\\s|\\*\\s|\\.)(References)\",\n",
    "        r\"(?i)(\\n|\\r\\n|\\r|\\.\\s|-\\s|\\*\\s|\\.)(Bibliography)\",\n",
    "        r\"(?i)(\\n|\\r\\n|\\r|\\.\\s|-\\s|\\*\\s|\\.)(Acknowledgements)\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, parsed_text)\n",
    "        if match:\n",
    "            return match.start() + len(match.group(1))\n",
    "    return -1\n",
    "\n",
    "\n",
    "def process_paper(pdf_link):\n",
    "    if download_pdf(pdf_link):\n",
    "        with open(\"temp.pdf\", \"rb\") as pdf_file:\n",
    "            text = extract_pdf_text(pdf_file)\n",
    "        os.remove(\"temp.pdf\")\n",
    "        text = text[: find_references_start(text)]\n",
    "        text = remove_surrogates(text)\n",
    "        return text.replace(\"\\n\", \"\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def append_id_floats_to_file(id_value, float_list, filename=\"f.txt\"):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, \"a\") as file:\n",
    "        if file_exists:\n",
    "            file.write(\"\\n\")\n",
    "        file.write(f\"{id_value}:\")\n",
    "        for num in float_list:\n",
    "            file.write(f\" {num}\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def read_id_floats_from_file(filename):\n",
    "    id_float_dict = {}\n",
    "    try:\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    parts = line.split(\":\")\n",
    "                    if len(parts) == 2:\n",
    "                        id_value = parts[0].strip()\n",
    "                        float_strings = parts[1].strip().split()\n",
    "                        float_list = [float(num) for num in float_strings]\n",
    "                        id_float_dict[id_value] = float_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing file: {e}\")\n",
    "    return id_float_dict\n",
    "\n",
    "\n",
    "result = read_id_floats_from_file(\"f2.txt\")\n",
    "\n",
    "\n",
    "for s in tqdm(submissions):\n",
    "    try:\n",
    "        pdf_link = f\"https://openreview.net/{s.content['pdf']['value']}\"\n",
    "        processed_text = process_paper(pdf_link)\n",
    "        emb = ollama.embeddings(\n",
    "            model=\"qwen2\",\n",
    "            prompt=processed_text,\n",
    "        )\n",
    "        embedding = emb[\"embedding\"]\n",
    "        paper_id = s.id\n",
    "        append_id_floats_to_file(paper_id, embedding, \"f2.txt\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(paper_id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae642db1-6f0a-48b3-9354-637fa282df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import random\n",
    "import re\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_id_floats_from_file(filename):\n",
    "    id_float_dict = {}\n",
    "    try:\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    parts = line.split(\":\")\n",
    "                    if len(parts) == 2:\n",
    "                        id_value = parts[0].strip()\n",
    "                        float_strings = parts[1].strip().split()\n",
    "                        float_list = [float(num) for num in float_strings]\n",
    "                        id_float_dict[id_value] = float_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing file: {e}\")\n",
    "    return id_float_dict\n",
    "\n",
    "\n",
    "result = read_id_floats_from_file(\"f2.txt\")\n",
    "\n",
    "ids = list(result.keys())\n",
    "id2title = {s.id: s.content[\"title\"][\"value\"] for s in submissions}\n",
    "id2abstract = {s.id: s.content[\"abstract\"][\"value\"] for s in submissions}\n",
    "labels = [id2title[id] for id in ids]\n",
    "embedding = np.array(list(result.values()))\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=15)\n",
    "cluster_labels = clusterer.fit_predict(embedding)\n",
    "clusters = {}\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(\n",
    "        {\"id\": ids[i], \"abstract\": id2abstract[ids[i]], \"title\": id2title[ids[i]]}\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cluster_label(\n",
    "    cluster_papers: List[Dict],\n",
    "    initial_label: Dict = None,\n",
    "    model: str = \"qwen2\",\n",
    "    sample_size: int = 500,\n",
    ") -> Dict:\n",
    "    sampled_papers = random.sample(\n",
    "        cluster_papers, min(len(cluster_papers), sample_size)\n",
    "    )\n",
    "    papers_text = \"\\n\".join(\n",
    "        [\n",
    "            f\"Title: {p['title']}\\nAbstract: {p['abstract'][:200]}...\"\n",
    "            for p in sampled_papers[:5]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"cluster_name\": \"Unique, specific technical name (4-7 words, avoid generic terms like 'Advanced' or 'Innovative')\",\n",
    "        \"explanation\": \"Detailed explanation of the cluster theme, specific methodologies, and key applications (2-3 sentences, include quantitative information if possible)\",\n",
    "        \"distinguishing_factor\": \"What makes this cluster unique compared to other clusters? Mention 1-2 most similar clusters and how this cluster differs (1-2 sentences)\",\n",
    "        \"sub_themes\": \"List 3 distinct sub-themes within this cluster, each with a brief (5-10 words) explanation\",\n",
    "    }\n",
    "\n",
    "    action = \"refining\" if initial_label else \"creating\"\n",
    "    initial_context = \"the initial label and \" if initial_label else \"\"\n",
    "    system_prompt = (\n",
    "        f\"You are {action} a cluster label for ICML 2024 papers. Given {initial_context}a list of papers, \"\n",
    "        f\"keywords, and a summary of all clusters, provide a unique and specific label for this cluster. \"\n",
    "        f\"Focus on the most distinctive aspects and avoid generic terms. Include precise methodologies \"\n",
    "        f\"and applications in your explanation. Clearly differentiate this cluster from others, especially \"\n",
    "        f\"the most similar ones. Output in JSON format matching this schema: {json.dumps(schema, indent=2)}\"\n",
    "    )\n",
    "\n",
    "    initial_label_text = (\n",
    "        f\"Initial Label:\\n{json.dumps(initial_label, indent=2)}\\n\"\n",
    "        if initial_label\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{'Refine' if initial_label else 'Create'} a label for this cluster:\\n\\n\"\n",
    "        f\"{initial_label_text}\\n\"\n",
    "        f\"Papers from this cluster:\\n{papers_text}\\n\\n\"\n",
    "        f\"Ensure your cluster name is unique and highly specific. The explanation should include \"\n",
    "        f\"precise methodologies and applications.\"\n",
    "        f\"Clearly state how this cluster differs from the 1-2 most similar clusters. \"\n",
    "        f\"Sub-themes should be distinct aspects within the cluster, not restatements of the main theme. \"\n",
    "        f\"Return the cluster information in the exact JSON format specified.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            system=system_prompt,\n",
    "            prompt=user_prompt,\n",
    "            options={\"num_ctx\": 131072},\n",
    "        )\n",
    "        return json.loads(response[\"response\"])\n",
    "    except json.JSONDecodeError:\n",
    "        match = re.search(r\"\\{.*\\}\", response[\"response\"], re.DOTALL)\n",
    "        return json.loads(match.group(0))\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"cluster_name\": \"Error\",\n",
    "            \"explanation\": str(e),\n",
    "            \"distinguishing_factor\": \"N/A\",\n",
    "            \"sub_themes\": [],\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Performing initial cluster labeling...\")\n",
    "initial_labels = {\n",
    "    label: get_cluster_label(papers) for label, papers in clusters.items()\n",
    "}\n",
    "\n",
    "print(\"\\nRefining cluster labels...\")\n",
    "refined_labels = {\n",
    "    label: get_cluster_label(papers, initial_labels[label])\n",
    "    for label, papers in clusters.items()\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Cluster Labels:\")\n",
    "for label, info in refined_labels.items():\n",
    "    print(f\"\\nCluster {label}:\")\n",
    "    print(f\"Name: {info['cluster_name']}\")\n",
    "    print(f\"Explanation: {info['explanation']}\")\n",
    "    print(f\"Distinguishing Factor: {info['distinguishing_factor']}\")\n",
    "    print(f\"Sub-themes: {info['sub_themes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import ollama\n",
    "\n",
    "\n",
    "def get_cluster_label(\n",
    "    cluster_papers: List[Dict],\n",
    "    initial_label: Dict = None,\n",
    "    parent_label: Dict = None,\n",
    "    model: str = \"qwen2\",\n",
    "    sample_size: int = 500,\n",
    ") -> Dict:\n",
    "    sampled_papers = random.sample(\n",
    "        cluster_papers, min(len(cluster_papers), sample_size)\n",
    "    )\n",
    "    papers_text = \"\\n\".join(\n",
    "        [\n",
    "            f\"Title: {p['title']}\\nAbstract: {p['abstract'][:200]}...\"\n",
    "            for p in sampled_papers[:5]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"cluster_name\": \"Unique, specific technical name (2-5 words, avoid generic terms like 'Advanced' or 'Innovative')\",\n",
    "        \"explanation\": \"Detailed explanation of the cluster theme, specific methodologies, and key applications (1-2 sentences)\",\n",
    "        \"distinguishing_factor\": \"What makes this cluster unique compared to other clusters? Mention 1-2 most similar clusters and how this cluster differs (1-2 sentences)\",\n",
    "    }\n",
    "\n",
    "    action = \"refining\" if initial_label else \"creating\"\n",
    "    initial_context = \"the initial label and \" if initial_label else \"\"\n",
    "    parent_context = \"the parent cluster label and \" if parent_label else \"\"\n",
    "\n",
    "    system_prompt = (\n",
    "        f\"You are {action} a {'sub-' if parent_label else ''}cluster label for ICML 2024 papers. \"\n",
    "        f\"Given {initial_context}{parent_context}a list of papers, provide a unique and specific label for this cluster. \"\n",
    "        f\"The name should make sense and is not a mashup of several categories.\"\n",
    "        f\"Focus on the most distinctive aspects and avoid generic terms. Include precise methodologies \"\n",
    "        f\"and applications in your explanation. Clearly differentiate this cluster from others, especially \"\n",
    "        f\"the most similar ones. Output in JSON format matching this schema: {json.dumps(schema, indent=2)}\"\n",
    "    )\n",
    "\n",
    "    initial_label_text = (\n",
    "        f\"Initial Label:\\n{json.dumps(initial_label, indent=2)}\\n\"\n",
    "        if initial_label\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    parent_label_text = (\n",
    "        f\"Parent Cluster Label:\\n{json.dumps(parent_label, indent=2)}\\n\"\n",
    "        if parent_label\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"{'Refine' if initial_label else 'Create'} a label for this {'sub-' if parent_label else ''}cluster:\\n\\n\"\n",
    "        f\"{initial_label_text}\\n\"\n",
    "        f\"{parent_label_text}\\n\"\n",
    "        f\"Papers from this cluster:\\n{papers_text}\\n\\n\"\n",
    "        f\"Ensure your cluster name is specific. The explanation should include \"\n",
    "        f\"precise methodologies and applications. Do not mash up names in the cluster label. \"\n",
    "        f\"Clearly state how this cluster differs from the 1-2 most similar clusters\"\n",
    "        f\"{' and how it relates to the parent cluster' if parent_label else ''}. \"\n",
    "        f\"Return the cluster information in the exact JSON format specified.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            system=system_prompt,\n",
    "            prompt=user_prompt,\n",
    "            options={\"num_ctx\": 131072},\n",
    "        )\n",
    "        return json.loads(response[\"response\"])\n",
    "    except json.JSONDecodeError:\n",
    "        match = re.search(r\"\\{.*\\}\", response[\"response\"], re.DOTALL)\n",
    "        return json.loads(match.group(0))\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"cluster_name\": \"Error\",\n",
    "            \"explanation\": str(e),\n",
    "            \"distinguishing_factor\": \"N/A\",\n",
    "            \"sub_themes\": [],\n",
    "        }\n",
    "\n",
    "\n",
    "def create_and_label_subclusters(cluster_papers, parent_label, embedding):\n",
    "    print(f\"Creating sub-clusters for cluster: {parent_label['cluster_name']}\")\n",
    "\n",
    "    # Extract embeddings for this cluster\n",
    "    cluster_ids = [paper[\"id\"] for paper in cluster_papers]\n",
    "    cluster_embedding = np.array([embedding[id] for id in cluster_ids])\n",
    "\n",
    "    # Create sub-clusters\n",
    "    sub_clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=1)\n",
    "    sub_cluster_labels = sub_clusterer.fit_predict(cluster_embedding)\n",
    "\n",
    "    sub_clusters = {}\n",
    "    for i, label in enumerate(sub_cluster_labels):\n",
    "        if label not in sub_clusters:\n",
    "            sub_clusters[label] = []\n",
    "        sub_clusters[label].append(cluster_papers[i])\n",
    "\n",
    "    # Label sub-clusters\n",
    "    sub_cluster_labels = {}\n",
    "    for label, papers in sub_clusters.items():\n",
    "        print(f\"  Labeling sub-cluster {label} ({len(papers)} papers)\")\n",
    "        initial_label = get_cluster_label(papers, parent_label=parent_label)\n",
    "        refined_label = get_cluster_label(\n",
    "            papers, initial_label=initial_label, parent_label=parent_label\n",
    "        )\n",
    "        sub_cluster_labels[label] = refined_label\n",
    "\n",
    "    return sub_clusters, sub_cluster_labels\n",
    "\n",
    "\n",
    "result = read_id_floats_from_file(\"f2.txt\")\n",
    "ids = list(result.keys())\n",
    "id2title = {s.id: s.content[\"title\"][\"value\"] for s in submissions}\n",
    "id2abstract = {s.id: s.content[\"abstract\"][\"value\"] for s in submissions}\n",
    "labels = [id2title[id] for id in ids]\n",
    "embedding = np.array(list(result.values()))\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=15)\n",
    "cluster_labels = clusterer.fit_predict(embedding)\n",
    "clusters = {}\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(\n",
    "        {\"id\": ids[i], \"abstract\": id2abstract[ids[i]], \"title\": id2title[ids[i]]}\n",
    "    )\n",
    "\n",
    "print(\"Performing initial cluster labeling...\")\n",
    "initial_labels = {\n",
    "    label: get_cluster_label(papers) for label, papers in clusters.items()\n",
    "}\n",
    "\n",
    "print(\"\\nRefining cluster labels...\")\n",
    "refined_labels = {\n",
    "    label: get_cluster_label(papers, initial_labels[label])\n",
    "    for label, papers in clusters.items()\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Cluster Labels:\")\n",
    "for label, info in refined_labels.items():\n",
    "    print(f\"\\nCluster {label}:\")\n",
    "    print(f\"Name: {info['cluster_name']}\")\n",
    "    print(f\"Explanation: {info['explanation']}\")\n",
    "    print(f\"Distinguishing Factor: {info['distinguishing_factor']}\")\n",
    "\n",
    "print(\"\\nCreating and labeling sub-clusters...\")\n",
    "all_subclusters = {}\n",
    "all_subcluster_labels = {}\n",
    "\n",
    "for label, papers in clusters.items():\n",
    "    if len(papers) > 15:\n",
    "        sub_clusters, sub_cluster_labels = create_and_label_subclusters(\n",
    "            papers, refined_labels[label], result\n",
    "        )\n",
    "        all_subclusters[label] = sub_clusters\n",
    "        all_subcluster_labels[label] = sub_cluster_labels\n",
    "\n",
    "        print(f\"\\nSub-clusters for Cluster {label} with {len(papers)} papers:\")\n",
    "        for sub_label, sub_info in sub_cluster_labels.items():\n",
    "            print(f\"  Sub-cluster {sub_label}:\")\n",
    "            print(f\"  Name: {sub_info['cluster_name']}\")\n",
    "            print(f\"  Explanation: {sub_info['explanation']}\")\n",
    "            print(f\"  Distinguishing Factor: {sub_info['distinguishing_factor']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_labels = {int(k): v for k, v in refined_labels.items()}\n",
    "with open(\"refined_labels.json\", \"w\") as f:\n",
    "    json.dump(refined_labels, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eded1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2pdf = {s.id: s.content[\"pdf\"][\"value\"] for s in submissions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f38af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for cluster_id, cluster in all_subcluster_labels.items():\n",
    "    cluster_name = refined_labels[cluster_id][\"cluster_name\"]\n",
    "    res[cluster_name] = {}\n",
    "    for sub_cluster_id, sub_cluster in cluster.items():\n",
    "        papers = all_subclusters[cluster_id][sub_cluster_id]\n",
    "        papers = [{**p, **{\"pdf_link\": id2pdf[p[\"id\"]]}} for p in papers]\n",
    "        res[cluster_name][sub_cluster[\"cluster_name\"]] = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embed_data.json\", \"w\") as f:\n",
    "    json.dump(res, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subcluster_labels = {\n",
    "    int(k): {int(k1): v1 for k1, v1 in v.items()}\n",
    "    for k, v in all_subcluster_labels.items()\n",
    "}\n",
    "with open(\"all_subcluster_labels.json\", \"w\") as f:\n",
    "    json.dump(all_subcluster_labels, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f5-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
