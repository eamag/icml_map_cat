{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"llm_papers.json\") as json_file:\n",
    "    llm_papers = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Dict, Tuple, Optional\n",
    "from geopy.geocoders import Nominatim, GoogleV3\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "RATE_LIMIT = 1.1\n",
    "USER_AGENT = \"geocode_cache_llm_papers/1.0 (@gmail.com)\"\n",
    "CACHE_FILE = \"geocode_cache_llm_papers.json\"\n",
    "MAP_FILE = \"papers_map.html\"\n",
    "\n",
    "geolocator = Nominatim(user_agent=USER_AGENT)\n",
    "google_geolocator = GoogleV3(api_key=\"\")\n",
    "\n",
    "\n",
    "def load_cache() -> Dict:\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cache(cache: Dict) -> None:\n",
    "    with open(CACHE_FILE, \"w\") as f:\n",
    "        json.dump(cache, f, indent=4)\n",
    "\n",
    "\n",
    "def geocode_with_fallback(\n",
    "    query: str, cache: Dict, address: Optional[str] = None\n",
    ") -> Tuple[Optional[Tuple[float, float]], Optional[str]]:\n",
    "    if query not in cache:\n",
    "        cache[query] = {\"address\": None, \"location\": None}\n",
    "\n",
    "    if cache[query][\"location\"] is None:\n",
    "        try:\n",
    "            if address:\n",
    "                location = geolocator.geocode(address) or google_geolocator.geocode(\n",
    "                    address\n",
    "                )\n",
    "            else:\n",
    "                location = geolocator.geocode(query)\n",
    "                if location is not None and not verify_location(\n",
    "                    query, (location.latitude, location.longitude)\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"Location verification failed for {query}. Searching for correct address...\"\n",
    "                    )\n",
    "                    correct_address = find_correct_address(query)\n",
    "                    location, _ = geocode_with_fallback(query, cache, correct_address)\n",
    "\n",
    "            if location:\n",
    "                cache[query][\"location\"] = (location.latitude, location.longitude)\n",
    "                if address:\n",
    "                    cache[query][\"address\"] = address\n",
    "            else:\n",
    "                cache[query][\"location\"] = None\n",
    "\n",
    "            save_cache(cache)\n",
    "        except (GeocoderTimedOut, GeocoderServiceError, AttributeError) as e:\n",
    "            print(f\"Error geocoding {query}: {e}\")\n",
    "        time.sleep(RATE_LIMIT)\n",
    "\n",
    "    return cache[query][\"location\"], cache[query][\"address\"]\n",
    "\n",
    "\n",
    "def search_google(query: str) -> str:\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"https://www.google.com/search?q={encoded_query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            html = response.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text_content = soup.get_text(separator=\" \", strip=True)\n",
    "        kw = \"g.co/privacytools\"\n",
    "        privacy_tools_index = text_content.find(kw)\n",
    "        if privacy_tools_index != -1:\n",
    "            text_content = text_content[privacy_tools_index + len(kw) :]\n",
    "        lines = [line.strip() for line in text_content.split(\"\\n\") if line.strip()]\n",
    "        filtered_lines = [\n",
    "            line\n",
    "            for line in lines\n",
    "            if not line.startswith(\n",
    "                (\"https://\", \"http://\", \"www.\", \"Images\", \"Videos\", \"Maps\", \"News\")\n",
    "            )\n",
    "        ]\n",
    "        clean_text = \" \".join(filtered_lines)\n",
    "        return clean_text\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "\n",
    "def verify_location(query: str, location: Tuple[float, float]) -> bool:\n",
    "    try:\n",
    "        reverse_location = geolocator.reverse(f\"{location[0]}, {location[1]}\")\n",
    "        prompt = f\"\"\"Verify if the following address matches the query. Respond with 'yes' or 'no' only.\n",
    "        Query: {query}\n",
    "        Address: {str(reverse_location)}\n",
    "        \"\"\"\n",
    "        response = (\n",
    "            ollama.generate(model=\"gemma2\", prompt=prompt)[\"response\"].strip().lower()\n",
    "        )\n",
    "        return response == \"yes\"\n",
    "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "        print(f\"Error verifying location for {query}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def find_correct_address(query: str) -> str:\n",
    "    search_results = search_google(f\"{query} address\")\n",
    "    prompt = f\"\"\"Find the address for the query from the search output below. Be concise, only output the most important parts of the address. \n",
    "    Query: {query}\n",
    "    Search results: {search_results}\n",
    "    \"\"\"\n",
    "    return (\n",
    "        ollama.generate(model=\"gemma2\", prompt=prompt)[\"response\"]\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "def process_single_institution(inst: str, cache: Dict) -> Optional[Tuple[float, float]]:\n",
    "    query = inst\n",
    "    location, _ = geocode_with_fallback(query, cache)\n",
    "\n",
    "    if not location:\n",
    "        print(f\"Location not found for {query}. Searching for correct address...\")\n",
    "        correct_address = find_correct_address(query)\n",
    "        location, _ = geocode_with_fallback(query, cache, correct_address)\n",
    "\n",
    "    return location\n",
    "\n",
    "\n",
    "def process_single_paper(paper_id: str, authors_data: Dict, cache: Dict) -> Dict:\n",
    "    processed = {\"papers\": collections.defaultdict(list), \"unknown\": set()}\n",
    "\n",
    "    for author_info, inst_list in authors_data.items():\n",
    "        for inst in inst_list:\n",
    "            loc = process_single_institution(inst, cache)\n",
    "            if loc:\n",
    "                processed[\"papers\"][paper_id].append({\"name\": inst, \"location\": loc})\n",
    "            else:\n",
    "                processed[\"unknown\"].add(inst)\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "def create_map(processed: Dict) -> None:\n",
    "    m = folium.Map(location=[0, 0], zoom_start=2)\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    for submission_id, locations in processed[\"papers\"].items():\n",
    "        for loc in locations:\n",
    "            folium.Marker(\n",
    "                loc[\"location\"],\n",
    "                popup=f\"<b>{loc['name']}</b><br>Paper: {submission_id}\",\n",
    "                tooltip=f\"Paper: {submission_id}\",\n",
    "            ).add_to(marker_cluster)\n",
    "\n",
    "    m.save(MAP_FILE)\n",
    "    return m\n",
    "\n",
    "\n",
    "def print_summary(processed: Dict) -> None:\n",
    "    print(f\"Papers: {len(processed['papers'])}\")\n",
    "    print(\n",
    "        f\"Mapped locations: {sum(len(locs) for locs in processed['papers'].values())}\"\n",
    "    )\n",
    "    print(f\"Unknown locations: {len(processed['unknown'])}\")\n",
    "    print(f\"Map saved as {MAP_FILE}\")\n",
    "\n",
    "\n",
    "def load_processed_data() -> Dict:\n",
    "    try:\n",
    "        with open(\"processed_papers_cache.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except (FileNotFoundError, pickle.PickleError):\n",
    "        return {\"papers\": collections.defaultdict(list), \"unknown\": set()}\n",
    "\n",
    "\n",
    "def save_processed_data(data: Dict) -> None:\n",
    "    with open(\"processed_papers_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def process_all_papers(res: Dict) -> Dict:\n",
    "    all_processed = load_processed_data()\n",
    "    unique_papers = set(res.keys())\n",
    "\n",
    "    already_processed = set(all_processed[\"papers\"].keys())\n",
    "    to_process = unique_papers - already_processed\n",
    "    cache = load_cache()\n",
    "\n",
    "    for paper_id in tqdm(to_process):\n",
    "        paper_processed = process_single_paper(paper_id, res[paper_id], cache)\n",
    "        all_processed[\"papers\"].update(paper_processed[\"papers\"])\n",
    "        all_processed[\"unknown\"].update(paper_processed[\"unknown\"])\n",
    "        save_processed_data(all_processed)\n",
    "\n",
    "    return all_processed\n",
    "\n",
    "\n",
    "def main(res: Dict) -> None:\n",
    "    processed = process_all_papers(res)\n",
    "    create_map(processed)\n",
    "    print_summary(processed)\n",
    "\n",
    "\n",
    "main(llm_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def find_best_match(institution, locations_data, threshold=80):\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "\n",
    "    for location in locations_data:\n",
    "        score = fuzz.ratio(institution.lower(), location.lower())\n",
    "        if score > best_score and score >= threshold:\n",
    "            best_score = score\n",
    "            best_match = location\n",
    "\n",
    "    return best_match\n",
    "\n",
    "\n",
    "def process_data(papers_file, locations_file):\n",
    "    # Load the papers data\n",
    "    with open(papers_file, \"r\") as f:\n",
    "        papers_data = json.load(f)\n",
    "\n",
    "    # Load the locations data\n",
    "    with open(locations_file, \"r\") as f:\n",
    "        locations_data = json.load(f)\n",
    "\n",
    "    # Create a dictionary to store institution -> papers mapping\n",
    "    institution_papers = {}\n",
    "\n",
    "    # Process the papers data\n",
    "    for paper_id, authors in papers_data.items():\n",
    "        for author, institutions in authors.items():\n",
    "            for institution in institutions:\n",
    "                if institution not in institution_papers:\n",
    "                    institution_papers[institution] = []\n",
    "                institution_papers[institution].append(paper_id)\n",
    "\n",
    "    # Combine with location data using fuzzy matching\n",
    "    result = {}\n",
    "    unmatched = []\n",
    "\n",
    "    for institution, papers in tqdm(institution_papers.items()):\n",
    "        best_match = find_best_match(institution, locations_data)\n",
    "\n",
    "        if best_match:\n",
    "            if best_match not in result:\n",
    "                result[best_match] = {\n",
    "                    \"papers\": papers,\n",
    "                    \"location\": locations_data[best_match][\"location\"],\n",
    "                }\n",
    "            else:\n",
    "                result[best_match][\"papers\"].extend(papers)\n",
    "        else:\n",
    "            unmatched.append(institution)\n",
    "\n",
    "    # Print unmatched institutions\n",
    "    if unmatched:\n",
    "        print(f\"Unmatched institutions: ({len(unmatched)})\")\n",
    "        for inst in unmatched:\n",
    "            print(f\"- {inst}\")\n",
    "\n",
    "    return result, unmatched\n",
    "\n",
    "\n",
    "# Example usage\n",
    "papers_file = \"papers.json\"\n",
    "locations_file = \"locations.json\"\n",
    "processed_data, unmatched = process_data(papers_file, locations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {}\n",
    "wrong_loc = []\n",
    "for k, v in processed_data.items():\n",
    "    if v[\"location\"] is not None:\n",
    "        markers[k] = {\n",
    "            k1: list(set(v1)) if k1 == \"papers\" else v1 for k1, v1 in v.items()\n",
    "        }\n",
    "    else:\n",
    "        wrong_loc.append([k, v])\n",
    "# Save the processed data to a new JSON file\n",
    "with open(\"markers.json\", \"w\") as f:\n",
    "    json.dump(markers, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
